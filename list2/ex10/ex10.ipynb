{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 10 - Exercise statement\n",
    " Plot the detected communities in the US airport network and the Neural Network using each of the methods: Louvain, Girvan Newman, Fast Greedy and Label Propagation. Analyse the differences among the plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 10 - Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the libraries that we'll use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from networkx.algorithms import community\n",
    "from community import community_louvain\n",
    "from networkx.algorithms.community import greedy_modularity_communities\n",
    "from networkx.algorithms.community import label_propagation_communities\n",
    "from networkx.algorithms.community import asyn_lpa_communities\n",
    "from networkx.algorithms.community.quality import modularity\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy.linalg import expm\n",
    "import pandas as pd\n",
    "import time\n",
    "from IPython.display import display, HTML, display_pretty\n",
    "#import warnings\n",
    "#warnings.simplefilter(\"ignore\")\n",
    "\n",
    "# A flag to use timer, just to prevent your system to freeze while running heavy processes\n",
    "useTimer = False #this feature is disabled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add some functions to help us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the n-th moment of a probability distribution\n",
    "def nth_moment_of_probability_distribution(xList, pxList, n):\n",
    "    return expected_value([x**n for x in xList], pxList)\n",
    "\n",
    "# Calculate the expected value of a random variable\n",
    "def expected_value(xList, pxList):\n",
    "    return sum([xList[i]*pxList[i] for i in range(len(xList))])\n",
    "\n",
    "# Calculate the shannon entropy of an array of probabilities\n",
    "def shannon_entropy(pxList):\n",
    "    H = 0\n",
    "    for p in pxList:\n",
    "        if(p > 0):\n",
    "            H = H - p*math.log(p, 2)\n",
    "    return H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a class to encapsulate and do the graph operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "\n",
    "    def __init__(self, name=None):\n",
    "        self.name = name\n",
    "\n",
    "    # Read graph from file and apply transformations\n",
    "    def read_graph(self, inputFile):\n",
    "        # To read the network from a file, we use the command read_edgelist.\n",
    "        G= nx.read_edgelist(inputFile, comments='%', nodetype=int, data=(('weight',float),))\n",
    "        # We transfor the network into the undirected version.\n",
    "        G = G.to_undirected()\n",
    "        # Here we consider only the largest component.\n",
    "        Gcc=sorted(nx.connected_component_subgraphs(G), key = len, reverse=True)\n",
    "        G=Gcc[0]\n",
    "        # Sometimes the node labels are not in the sequential order or strings are used. To facilitate our implementation, let us convert the labels to integers starting with the index zero, because Python uses 0-based indexing.\n",
    "        G = nx.convert_node_labels_to_integers(G, first_label=0)\n",
    "        # Save graph to the network\n",
    "        self.graph = G\n",
    "        return self.graph\n",
    "\n",
    "    # Read graph from file and apply transformations\n",
    "    def read_gml(self, inputFile):\n",
    "        # To read the network from a file, we use the command read_edgelist.\n",
    "        G = nx.read_gml(inputFile)\n",
    "        # We transfor the network into the undirected version.\n",
    "        G = G.to_undirected()\n",
    "        # Here we consider only the largest component.\n",
    "        Gcc=sorted(nx.connected_component_subgraphs(G), key = len, reverse=True)\n",
    "        G=Gcc[0]\n",
    "        # Sometimes the node labels are not in the sequential order or strings are used. To facilitate our implementation, let us convert the labels to integers starting with the index zero, because Python uses 0-based indexing.\n",
    "        G = nx.convert_node_labels_to_integers(G, first_label=0)\n",
    "        # Save graph to the network\n",
    "        self.graph = G\n",
    "        return self.graph\n",
    "\n",
    "    # Turn multigraph into graph\n",
    "    def multiGraphToGraph(self):\n",
    "        self.graph = nx.Graph(self.graph)\n",
    "\n",
    "    # Store and return the transitivity of the graph\n",
    "    def transitivity(self):\n",
    "        return nx.transitivity(self.graph)\n",
    "\n",
    "    # Store and return the average clustering coefficient of the graph\n",
    "    def average_clustering(self):\n",
    "        return nx.average_clustering(self.graph)\n",
    "\n",
    "    # Get number of edges of the graph\n",
    "    def number_of_edges(self):\n",
    "        return self.graph.number_of_edges()\n",
    "\n",
    "    # Get number of nodes of the graph\n",
    "    def number_of_nodes(self):\n",
    "        return len(self.graph)\n",
    "\n",
    "    # Calculate average node degree\n",
    "    def average_node_degree(self):\n",
    "        # Get the degrees\n",
    "        degrees = [d for n,d in self.graph.degree()]\n",
    "        # Calculate the average\n",
    "        return sum(degrees)/len(degrees)\n",
    "\n",
    "\n",
    "    # Calculate the nth moment of degree distribution\n",
    "    def nth_moment_of_degree_distribution(self, n):\n",
    "        kvalues, pk = self.degree_distribution()\n",
    "        return nth_moment_of_probability_distribution(kvalues, pk, n)\n",
    "\n",
    "    # Calculate average shortest path length\n",
    "    def average_shortest_path_length(self, weight=None):\n",
    "        return nx.average_shortest_path_length(self.graph, weight)\n",
    "\n",
    "    # Calculate diameter\n",
    "    def diameter(self, e=None):\n",
    "        return nx.diameter(self.graph, e)\n",
    "\n",
    "    # Calculate assortativity coefficient\n",
    "    def degree_assortativity_coefficient(self):\n",
    "        return nx.degree_assortativity_coefficient(self.graph)\n",
    "\n",
    "    def modularity(self, communities):\n",
    "        return modularity(self.graph, communities)\n",
    "\n",
    "    # Calculate degree distribution\n",
    "    def degree_distribution(self):\n",
    "        vk = dict(self.graph.degree())\n",
    "        vk = list(vk.values())  # we get only the degree values\n",
    "        vk = np.array(vk)\n",
    "        maxk = np.max(vk)\n",
    "        mink = np.min(vk)\n",
    "        kvalues= np.arange(0,maxk+1) # possible values of k\n",
    "        Pk = np.zeros(maxk+1) # P(k)\n",
    "        for k in vk:\n",
    "            Pk[k] = Pk[k] + 1\n",
    "        Pk = Pk/sum(Pk) # the sum of the elements of P(k) must to be equal to one\n",
    "        return (kvalues, Pk)\n",
    "\n",
    "\n",
    "    # Plot degree distribution\n",
    "    def plot_degree_distribution(self):\n",
    "        degree_distribution = self.degree_distribution()\n",
    "        fig = plt.subplot(1,1,1)\n",
    "        fig.set_xscale('log')\n",
    "        fig.set_yscale('log')\n",
    "        plt.suptitle(self.name, fontsize=16)\n",
    "        plt.plot(degree_distribution[0], degree_distribution[1],'bo')\n",
    "        plt.xlabel(\"k\", fontsize=20)\n",
    "        plt.ylabel(\"P(k)\", fontsize=20)\n",
    "        plt.title('Degree distribution', fontsize=20)\n",
    "        plt.show(block=True)\n",
    "        plt.clf()\n",
    "\n",
    "    # Plot the graph\n",
    "    def plot_graph(self):\n",
    "        plt.figure(figsize=(10,10))\n",
    "        pos=nx.spring_layout(self.graph)\n",
    "        nx.draw(self.graph, with_labels = True, pos = pos)\n",
    "        plt.show(block=True)\n",
    "        plt.clf()\n",
    "\n",
    "# Generate a graph for girvan newman benchmark\n",
    "def benchmark_girvan_newman():\n",
    "    N = 128\n",
    "    tau1 = 3\n",
    "    tau2 = 1.5\n",
    "    mu = 0.04\n",
    "    k =16\n",
    "    minc = 32\n",
    "    maxc = 32\n",
    "    return LFR_benchmark_graph(n = N, tau1 = tau1, tau2 = tau2, mu = mu, min_degree = k,\n",
    "                            max_degree = k, min_community=minc, max_community = maxc, seed = 10)\n",
    "\n",
    "# Louvain's community detection method\n",
    "def detect_communities_louvain(G):\n",
    "    partition = community_louvain.best_partition(G)\n",
    "    communities = list()\n",
    "    for com in set(partition.values()) :\n",
    "        list_nodes = [nodes for nodes in partition.keys() if partition[nodes] == com]\n",
    "        communities.append(sorted(list_nodes))\n",
    "    return sorted(communities)\n",
    "\n",
    "# Girvan Newman's community detection method\n",
    "def detect_communities_girvan_newman(G):\n",
    "    communities = community.girvan_newman(G)\n",
    "    return sorted(sorted(c) for c in next(communities))\n",
    "\n",
    "# Fast Greedy community detection method\n",
    "def detect_communities_greedy(G):\n",
    "    communities = greedy_modularity_communities(G)\n",
    "    return sorted(map(sorted, communities))\n",
    "\n",
    "# Label propagation community detection method\n",
    "def detect_communities_label_propagation(G):\n",
    "    communities = list()\n",
    "    #for c in asyn_lpa_communities(G):\n",
    "    for c in label_propagation_communities(G):\n",
    "        communities.append(sorted(c))\n",
    "    return sorted(communities)\n",
    "\n",
    "# Plot graph with communities, receives a list of communities, where each community is a list of nodes (ints)\n",
    "def show_communities(G, communities, name='title'):\n",
    "\n",
    "    pos=nx.spring_layout(G)\n",
    "    colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k', 'w']\n",
    "    plt.figure()\n",
    "    plt.title(name, fontsize=20)\n",
    "    for i in range(len(communities)):\n",
    "        if(i < len(colors)):\n",
    "            nx.draw_networkx_nodes(G, pos, communities[i], node_size = 50, node_color = colors[i])\n",
    "        else:\n",
    "            nx.draw_networkx_nodes(G, pos, communities[i], node_size = 50, node_color = np.random.rand(3,))\n",
    "\n",
    "    nx.draw_networkx_edges(G, pos, alpha=0.5)\n",
    "    plt.show(block=True)\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    # This line is a strange hack to prevent jupyter from sending a warning called:\n",
    "    # [IPKernelApp] WARNING | WARNING: attempted to send message from fork\n",
    "    # On github there is a similar issue still opened: <https://github.com/tqdm/tqdm/issues/485>\n",
    "    print(' ', end='', flush=True)\n",
    "\n",
    "    # List of files to open\n",
    "    networkFiles = [\n",
    "                    #'data/out.ca-AstroPh',\n",
    "                    #'data/out.ego-facebook',\n",
    "                    #'data/out.petster-friendships-hamster-uniq',\n",
    "                    #'data/out.subelj_euroroad_euroroad',\n",
    "                    'data/USairport500.txt'\n",
    "                    #'data/out.maayan-vidal'\n",
    "    ]\n",
    "\n",
    "    # List of names of the networks\n",
    "    networkNames = [\n",
    "                    #'ArXiv’s Astrophysics',\n",
    "                    #'Facebook user-user friendships',\n",
    "                    #'Hamsterster friendships',\n",
    "                    #'E-road network',\n",
    "                    'US airport'\n",
    "                    #'Human protein'\n",
    "    ]\n",
    "\n",
    "    # Load all networks\n",
    "\n",
    "    # List of networks\n",
    "    networks = []\n",
    "    for i in range(len(networkNames)):\n",
    "        # Load network\n",
    "        networks.append(Network(name=networkNames[i]))\n",
    "        networks[i].read_graph(networkFiles[i])\n",
    "    # Read glm network\n",
    "    networkNames.append('Neural Network')\n",
    "    multiGraph =  Network(name=networkNames[-1])\n",
    "    multiGraph.read_gml('data/celegansneural.gml')\n",
    "    multiGraph.multiGraphToGraph()\n",
    "    networks.append(multiGraph)\n",
    "\n",
    "    # List of method names\n",
    "    methodNames = [\n",
    "        'Louvain',\n",
    "        'Girvan Newman',\n",
    "        'Fast Greedy',\n",
    "        'Label Propagation'\n",
    "    ]\n",
    "\n",
    "    # List of community detection methods\n",
    "    methods = [ detect_communities_louvain,\n",
    "    detect_communities_girvan_newman,\n",
    "    detect_communities_greedy,\n",
    "    detect_communities_label_propagation\n",
    "    ]\n",
    "\n",
    "    # For each network\n",
    "    for i in range(len(networkNames)):\n",
    "        # Get current network\n",
    "        network = networks[i]\n",
    "\n",
    "        for j in range(len(methodNames)):\n",
    "            # Gets community from graph using method[j]\n",
    "            community = methods[j](network.graph)\n",
    "            # Make plot title\n",
    "            plotTitle = networkNames[i] + ' - ' + methodNames[j]\n",
    "            # Plot graph with its communities and name it\n",
    "            #show_communities(G, result, name=method.__name__[19:])\n",
    "            show_communities(network.graph, community, name=plotTitle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a process to run the main function. The advantage of using this strategy, instead of directly invoking main(), is that it gives us more control to stop the program.\n",
    " If we set the useTimer variable at the beginning of the program to True, the timer will stop the main process after 60 seconds. It's useful when we hit Ctrl+C in the terminal and the program refuses to stop.\n",
    " Also, it's important to catch the KeyboardInterrupt exception, that is raised when we hit Ctrl+C.\n",
    " By default, Ctrl+C would kill our program and the process that we've created would still continue to run.\n",
    " When we catch the exception, we also terminate the process of the function main. This way, Ctrl+C works again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Process\n",
    "from threading import Timer\n",
    "if __name__ == \"__main__\":\n",
    "    # Create the main process\n",
    "    proc = Process(target=main)\n",
    "\n",
    "    # Use timer if set\n",
    "    if(useTimer):\n",
    "        #declare timer, it gets the time in seconds\n",
    "        timer = Timer(300, proc.terminate)\n",
    "        timer.start()\n",
    "\n",
    "    try:\n",
    "        # Start process\n",
    "        proc.start()\n",
    "\n",
    "        # Block until process terminate\n",
    "        proc.join()\n",
    "    except KeyboardInterrupt:\n",
    "        proc.terminate()\n",
    "\n",
    "    # If process ends in time, cancel timer\n",
    "    if(useTimer):\n",
    "        timer.cancel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see the beautiful community detection with colors. For the Neural Network, it's visible that the Girvan Newman's method and the label propagation detection method had a bad modularity for the networks, because they classified almost all nodes in the same class. While with Louvain and Fast Greedy methods, we see well defined classes with reasonable modularity.\n",
    " Using the same criteria, it's visible when the methods had good modularity on the US airport network and when they didn't."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
