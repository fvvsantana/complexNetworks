{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 9\n",
    " **Note:** because of the Human Proteins network was taking too long to run, we didn't include this network in the computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the libraries that we'll use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from networkx.algorithms import community\n",
    "from community import community_louvain\n",
    "from networkx.algorithms.community import greedy_modularity_communities\n",
    "from networkx.algorithms.community import label_propagation_communities\n",
    "from networkx.algorithms.community import asyn_lpa_communities\n",
    "from networkx.algorithms.community.quality import modularity\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy.linalg import expm\n",
    "import pandas as pd\n",
    "import time\n",
    "from IPython.display import display, HTML, display_pretty\n",
    "#import warnings\n",
    "#warnings.simplefilter(\"ignore\")\n",
    "\n",
    "# A flag to use timer, just to prevent your system to freeze while running heavy processes\n",
    "useTimer = False #this feature is disabled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add some functions to help us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the n-th moment of a probability distribution\n",
    "def nth_moment_of_probability_distribution(xList, pxList, n):\n",
    "    return expected_value([x**n for x in xList], pxList)\n",
    "\n",
    "# Calculate the expected value of a random variable\n",
    "def expected_value(xList, pxList):\n",
    "    return sum([xList[i]*pxList[i] for i in range(len(xList))])\n",
    "\n",
    "# Calculate the shannon entropy of an array of probabilities\n",
    "def shannon_entropy(pxList):\n",
    "    H = 0\n",
    "    for p in pxList:\n",
    "        if(p > 0):\n",
    "            H = H - p*math.log(p, 2)\n",
    "    return H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a class to encapsulate and do the graph operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "\n",
    "    def __init__(self, name=None):\n",
    "        self.name = name\n",
    "\n",
    "    # Read graph from file and apply transformations\n",
    "    def read_graph(self, inputFile):\n",
    "        # To read the network from a file, we use the command read_edgelist.\n",
    "        G= nx.read_edgelist(inputFile, comments='%', nodetype=int, data=(('weight',float),))\n",
    "        # We transfor the network into the undirected version.\n",
    "        G = G.to_undirected()\n",
    "        # Here we consider only the largest component.\n",
    "        Gcc=sorted(nx.connected_component_subgraphs(G), key = len, reverse=True)\n",
    "        G=Gcc[0]\n",
    "        # Sometimes the node labels are not in the sequential order or strings are used. To facilitate our implementation, let us convert the labels to integers starting with the index zero, because Python uses 0-based indexing.\n",
    "        G = nx.convert_node_labels_to_integers(G, first_label=0)\n",
    "        # Save graph to the network\n",
    "        self.graph = G\n",
    "        return self.graph\n",
    "\n",
    "    # Read graph from file and apply transformations\n",
    "    def read_gml(self, inputFile):\n",
    "        # To read the network from a file, we use the command read_edgelist.\n",
    "        G = nx.read_gml(inputFile)\n",
    "        # We transfor the network into the undirected version.\n",
    "        G = G.to_undirected()\n",
    "        # Here we consider only the largest component.\n",
    "        Gcc=sorted(nx.connected_component_subgraphs(G), key = len, reverse=True)\n",
    "        G=Gcc[0]\n",
    "        # Sometimes the node labels are not in the sequential order or strings are used. To facilitate our implementation, let us convert the labels to integers starting with the index zero, because Python uses 0-based indexing.\n",
    "        G = nx.convert_node_labels_to_integers(G, first_label=0)\n",
    "        # Save graph to the network\n",
    "        self.graph = G\n",
    "        return self.graph\n",
    "\n",
    "    # Turn multigraph into graph\n",
    "    def multiGraphToGraph(self):\n",
    "        self.graph = nx.Graph(self.graph)\n",
    "\n",
    "    # Store and return the transitivity of the graph\n",
    "    def transitivity(self):\n",
    "        return nx.transitivity(self.graph)\n",
    "\n",
    "    # Store and return the average clustering coefficient of the graph\n",
    "    def average_clustering(self):\n",
    "        return nx.average_clustering(self.graph)\n",
    "\n",
    "    # Get number of edges of the graph\n",
    "    def number_of_edges(self):\n",
    "        return self.graph.number_of_edges()\n",
    "\n",
    "    # Get number of nodes of the graph\n",
    "    def number_of_nodes(self):\n",
    "        return len(self.graph)\n",
    "\n",
    "    # Calculate average node degree\n",
    "    def average_node_degree(self):\n",
    "        # Get the degrees\n",
    "        degrees = [d for n,d in self.graph.degree()]\n",
    "        # Calculate the average\n",
    "        return sum(degrees)/len(degrees)\n",
    "\n",
    "\n",
    "    # Calculate the nth moment of degree distribution\n",
    "    def nth_moment_of_degree_distribution(self, n):\n",
    "        kvalues, pk = self.degree_distribution()\n",
    "        return nth_moment_of_probability_distribution(kvalues, pk, n)\n",
    "\n",
    "    # Calculate average shortest path length\n",
    "    def average_shortest_path_length(self, weight=None):\n",
    "        return nx.average_shortest_path_length(self.graph, weight)\n",
    "\n",
    "    # Calculate diameter\n",
    "    def diameter(self, e=None):\n",
    "        return nx.diameter(self.graph, e)\n",
    "\n",
    "    # Calculate assortativity coefficient\n",
    "    def degree_assortativity_coefficient(self):\n",
    "        return nx.degree_assortativity_coefficient(self.graph)\n",
    "\n",
    "    def modularity(self, communities):\n",
    "        return modularity(self.graph, communities)\n",
    "\n",
    "    '''\n",
    "    # Girvan Newman's community detection method\n",
    "    def detect_communities_girvan_newman(self):\n",
    "        communities = community.girvan_newman(self.graph)\n",
    "        return sorted(sorted(c) for c in next(communities))\n",
    "    '''\n",
    "\n",
    "\n",
    "    # Calculate degree distribution\n",
    "    def degree_distribution(self):\n",
    "        vk = dict(self.graph.degree())\n",
    "        vk = list(vk.values())  # we get only the degree values\n",
    "        vk = np.array(vk)\n",
    "        maxk = np.max(vk)\n",
    "        mink = np.min(vk)\n",
    "        kvalues= np.arange(0,maxk+1) # possible values of k\n",
    "        Pk = np.zeros(maxk+1) # P(k)\n",
    "        for k in vk:\n",
    "            Pk[k] = Pk[k] + 1\n",
    "        Pk = Pk/sum(Pk) # the sum of the elements of P(k) must to be equal to one\n",
    "        return (kvalues, Pk)\n",
    "\n",
    "\n",
    "    # Plot degree distribution\n",
    "    def plot_degree_distribution(self):\n",
    "        degree_distribution = self.degree_distribution()\n",
    "        fig = plt.subplot(1,1,1)\n",
    "        fig.set_xscale('log')\n",
    "        fig.set_yscale('log')\n",
    "        plt.suptitle(self.name, fontsize=16)\n",
    "        plt.plot(degree_distribution[0], degree_distribution[1],'bo')\n",
    "        plt.xlabel(\"k\", fontsize=20)\n",
    "        plt.ylabel(\"P(k)\", fontsize=20)\n",
    "        plt.title('Degree distribution', fontsize=20)\n",
    "        plt.show(block=True)\n",
    "        plt.clf()\n",
    "\n",
    "    # Plot the graph\n",
    "    def plot_graph(self):\n",
    "        plt.figure(figsize=(10,10))\n",
    "        pos=nx.spring_layout(self.graph)\n",
    "        nx.draw(self.graph, with_labels = True, pos = pos)\n",
    "        plt.show(block=True)\n",
    "        plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Louvain's community detection method\n",
    "def detect_communities_louvain(G):\n",
    "    partition = community_louvain.best_partition(G)\n",
    "    communities = list()\n",
    "    for com in set(partition.values()) :\n",
    "        list_nodes = [nodes for nodes in partition.keys() if partition[nodes] == com]\n",
    "        communities.append(sorted(list_nodes))\n",
    "    return sorted(communities)\n",
    "\n",
    "# Girvan Newman's community detection method\n",
    "def detect_communities_girvan_newman(G):\n",
    "    communities = community.girvan_newman(G)\n",
    "    return sorted(sorted(c) for c in next(communities))\n",
    "\n",
    "# Fast Greedy community detection method\n",
    "def detect_communities_greedy(G):\n",
    "    communities = greedy_modularity_communities(G)\n",
    "    return sorted(map(sorted, communities))\n",
    "\n",
    "# Label propagation community detection method\n",
    "def detect_communities_label_propagation(G):\n",
    "    communities = list()\n",
    "    #for c in asyn_lpa_communities(G):\n",
    "    for c in label_propagation_communities(G):\n",
    "        communities.append(sorted(c))\n",
    "    return sorted(communities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll calculate the measures for each network. Then we store it in a dict to present it at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    # This line is a strange hack to prevent jupyter from sending a warning called:\n",
    "    # [IPKernelApp] WARNING | WARNING: attempted to send message from fork\n",
    "    # On github there is a similar issue still opened: <https://github.com/tqdm/tqdm/issues/485>\n",
    "    print(' ', end='', flush=True)\n",
    "\n",
    "    # List of files to open\n",
    "    networkFiles = [\n",
    "                    #'data/out.ca-AstroPh',\n",
    "                    #'data/out.ego-facebook',\n",
    "                    #'data/out.petster-friendships-hamster-uniq',\n",
    "                    'data/out.subelj_euroroad_euroroad',\n",
    "                    'data/USairport500.txt',\n",
    "                    #'data/out.maayan-vidal'\n",
    "    ]\n",
    "\n",
    "    # List of names of the networks\n",
    "    networkNames = [\n",
    "                    #'ArXivâ€™s Astrophysics',\n",
    "                    #'Facebook user-user friendships',\n",
    "                    #'Hamsterster friendships',\n",
    "                    'E-road network',\n",
    "                    'US airport',\n",
    "                    #'Human protein'\n",
    "    ]\n",
    "\n",
    "    # Load all networks\n",
    "\n",
    "    # List of networks\n",
    "    networks = []\n",
    "    for i in range(len(networkNames)):\n",
    "        # Load network\n",
    "        networks.append(Network(name=networkNames[i]))\n",
    "        networks[i].read_graph(networkFiles[i])\n",
    "    # Read glm network\n",
    "    networkNames.append('Neural Network')\n",
    "    multiGraph =  Network(name=networkNames[-1])\n",
    "    multiGraph.read_gml('data/celegansneural.gml')\n",
    "    multiGraph.multiGraphToGraph()\n",
    "    networks.append(multiGraph)\n",
    "\n",
    "    # List of community detection methods\n",
    "    methods = [\n",
    "        detect_communities_louvain,\n",
    "        detect_communities_girvan_newman,\n",
    "        detect_communities_greedy,\n",
    "        detect_communities_label_propagation\n",
    "    ]\n",
    "\n",
    "    # List of method names\n",
    "    methodNames = [\n",
    "        'Louvain',\n",
    "        'Girvan Newman',\n",
    "        'Fast Greedy',\n",
    "        'Label Propagation'\n",
    "    ]\n",
    "\n",
    "    # Create a dict to store the modularities for each method and each network\n",
    "    modularities = dict()\n",
    "    modularities['Network'] = []\n",
    "    for methodName in methodNames:\n",
    "        modularities[methodName] = []\n",
    "\n",
    "    # Dict with data to show\n",
    "    data = {'Network':[],\n",
    "            'Nodes':[],\n",
    "            'Average degree':[],\n",
    "            'Assortativity coefficient':[],\n",
    "            'Average shortest path length':[],\n",
    "            'Average clustering coefficient':[],\n",
    "            'Transitivity':[]\n",
    "            }\n",
    "\n",
    "    # For each network\n",
    "    for i in range(len(networkNames)):\n",
    "        # Get current network\n",
    "        network = networks[i]\n",
    "\n",
    "        # Append network name\n",
    "        data['Network'].append(networkNames[i])\n",
    "\n",
    "        # Get number of nodes\n",
    "        data['Nodes'].append(network.number_of_nodes())\n",
    "\n",
    "        # Get number of nodes\n",
    "        data['Average degree'].append(network.average_node_degree())\n",
    "\n",
    "        # Calculate assortativity coefficient\n",
    "        data['Assortativity coefficient'].append(network.degree_assortativity_coefficient())\n",
    "\n",
    "        # Calculate average shortest path length\n",
    "        data['Average shortest path length'].append(network.average_shortest_path_length())\n",
    "\n",
    "        # Calculate Average Clustering Cofficient\n",
    "        data['Average clustering coefficient'].append(network.average_clustering())\n",
    "\n",
    "        # Calculate transitivity\n",
    "        data['Transitivity'].append(network.transitivity())\n",
    "\n",
    "        # Append network name\n",
    "        modularities['Network'].append(networkNames[i])\n",
    "        for j in range(len(methodNames)):\n",
    "            # Gets community from graph using method[j]\n",
    "            community = methods[j](network.graph)\n",
    "            # Calculate modularity of community\n",
    "            modularities[methodNames[j]].append(network.modularity(community))\n",
    "\n",
    "        print(networkNames[i], ' calculated.')\n",
    "\n",
    "    # Display DataFrame\n",
    "    print('Measures by network')\n",
    "    df = pd.DataFrame(data)\n",
    "    display(df)\n",
    "\n",
    "    # Display DataFrame\n",
    "    print('Modularity by method')\n",
    "    df = pd.DataFrame(modularities)\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a process to run the main function. The advantage of using this strategy, instead of directly invoking main(), is that it gives us more control to stop the program.\n",
    " If we set the useTimer variable at the beginning of the program to True, the timer will stop the main process after 60 seconds. It's useful when we hit Ctrl+C in the terminal and the program refuses to stop.\n",
    " Also, it's important to catch the KeyboardInterrupt exception, that is raised when we hit Ctrl+C.\n",
    " By default, Ctrl+C would kill our program and the process that we've created would still continue to run.\n",
    " When we catch the exception, we also terminate the process of the function main. This way, Ctrl+C works again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Process\n",
    "from threading import Timer\n",
    "if __name__ == \"__main__\":\n",
    "    # Create the main process\n",
    "    proc = Process(target=main)\n",
    "\n",
    "    # Use timer if set\n",
    "    if(useTimer):\n",
    "        #declare timer, it gets the time in seconds\n",
    "        timer = Timer(300, proc.terminate)\n",
    "        timer.start()\n",
    "\n",
    "    try:\n",
    "        # Start process\n",
    "        proc.start()\n",
    "\n",
    "        # Block until process terminate\n",
    "        proc.join()\n",
    "    except KeyboardInterrupt:\n",
    "        proc.terminate()\n",
    "\n",
    "    # If process ends in time, cancel timer\n",
    "    if(useTimer):\n",
    "        timer.cancel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we run this code, we'll see that the average clustering coefficient and the transitivity diverge in some networks. As stated on Wikipedia:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the Louvain method had the best results in modularity among the tested methods. The Fast Greedy method also did a good job on the E-road and Neural networks, but in the US airport it had a bad performance.\n",
    " Indeed, the US airport network had the worst performance among all the networks. It was easier for most of the methods to detect the communities in the E-road network, because it's a big-world network (given the average shortest path), so you don't have edges crossing distant communities. For the US airport network and the Neural Network, the measures are almost the same. So, in order so try to explain it, we calculated the Average Clustering Coefficient and the Transitivity of these networks. As we can see, the average clustering coefficient and the transitivity are way bigger in the US airport than in the Neural Network. This level of clustering probably causes confusion in the algorithms because it's hard to separate the clusters.\n",
    " Regarding assortativity, observing the data, we can state that the higher the assortativity in a network, the higher is the modularity that the detection community algorithms can reach."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
